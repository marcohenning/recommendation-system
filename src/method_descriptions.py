

method_descriptions = {
    "Explanation Goodness Checklist (Checklist for Researchers)": "The Explanation Goodness Checklist offers a quick and cheap way to approximate the quality of explanations. Researchers can use it to make decontextualized judgements based on seven questions about the explanations that they have to answer with either \"yes\" or \"no\". Ideally, the researcher answering the questions should not be the creator of the system being assessed. The checklist requires no human subjects and no experience with the system, which makes it cheap, but since the quality of the explanations is being assessed without being used in the relevant context, the accuracy may be lower than that of other methods. \nMore information: Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance (2023)",
    "Explanation Satisfaction Scale": "The Explanation Satisfaction Scale is a version of the Explanation Goodness Checklist adapted to be used by users within the context of the application (after they have worked with the system). Each item in the questionnaire has to be rated on a Likert scale of 1-5. The items include questions about the explanation’s completeness, detail and their perceived usefulness. Since human subjects are required, this method is more costly and time-consuming than the Explanation Goodness Checklist, but the results are likely more accurate. \nMore information: Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance (2023)",
    "System Causability Scale (SCS)": "The System Causability Scale (SCS) is a questionnaire that aims to measure the general quality of explanations. It is inspired by the System Usability Scale (SUS), which is a popular way of quickly measuring the usability of a newly designed interface and has been in use for decades. The SCS uses the Likert scale and is made up of ten questions, inquiring about factors such as understandability, consistency and the timeliness of explanations. \nMore information: Measuring the Quality of Explanations: The System Causability Scale (SCS) (2020)",
    "Matching Human Explanations": "An automated way of objectively evaluating textual explanations by matching human explanations against computer-generated explanations using the three automated metrics BiLingual Evaluation Understudy (BLEU) that quantifies the similarity between sentences, Automatic NT Translation Metric (METEOR) that semantically calculates the similarity between words of sentences, and Consensus-based Image Description Evaluation (CIDEr) that compares sentences to human written references. A human-written dataset is required. \nMore information: A Survey of Evaluation Methods and Metrics for Explanations in Human–Robot Interaction (HRI) (2023)",
    "Interview": "Interviews offer researchers the possibility of probing selected aspects and getting deep insights into the user’s thoughts. The resulting qualitative data can be analyzed and used to gain a deeper understanding of areas that need to be improved compared to more shallow methods such as questionnaires. However, conducting a good interview requires skill at question design and interviewing, which makes the usefulness of this method highly dependent on the interviewer. Interviews are very versatile as they can be used to gain insights into perceived quality, user trust, user understanding of the system, or the timing and need of the explanations, depending on which questions are asked. \nMore information: Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance (2023)",
    "Trust Scale for Robots": "The Trust Scale for Robots is a questionnaire proposed by Schaefer to specifically to measure trust in robots. Human subjects interact with a robot and then complete a 40 item scale (or alternatively a 14 item subscale for more rapid trust measurement). The 14 item subscale focuses only on functional capabilities of the robot such as predictability, reliability, malfunctions and responsiveness, while the 40 item scale also inquires about very specific information (i.e., does it protect people, is it lifelike, or does it warn people of potential risks in the environment). If these specific questions do not apply to the robot in question, then the 14 item subscale may be used instead. At the end, a trust score is calculated to quantify the user’s trust in the robot. \nMore information: The Perception And Measurement Of Human-robot Trust The Perception And Measurement Of Human-robot Trust (2013)",
    "Likelihood of users following the agent’s suggestions": "A way of measuring trust is to test whether explanations increase how often a user follows the robot’s suggestions. A prerequisite for this is, of course, that the robot provides some sort of suggestions to the user (i.e., acting as a decision support-system). Accepting the robot’s suggestions more often when presented with explanations is a good indication of increased user trust. \nMore information: A Survey of Evaluation Methods and Metrics for Explanations in Human–Robot Interaction (HRI) (2023)",
    "Number of interactions between the user and the agent": "One can measure the number of interactions between human and robot to determine whether explanations increase this number. An increase in interactions is also a good indication of higher user trust. This method is easy to measure and does not rely on subjective user perceptions but only give very shallow insights without specific user feedback. \nMore information: A Survey of Evaluation Methods and Metrics for Explanations in Human–Robot Interaction (HRI) (2023)",
    "Domain expert experiment with the exact application task": "Domain expert experiments with the real task the application has been developed for are based on the idea that the best way to assess the quality of the explanations is to test the system with the exact task and target audience it was built for. The performance of human-agent team can objectively be measured with respect to the goal of the task, such as the accuracy of diagnoses or the rate of identifying errors made by the system. This evaluation method is difficult to perform as it requires high standards of experimental design and requires the budget to compensate domain experts but since it evaluates the system on the intended task with the intended users, it gives strong evidence of success. \nMore information: Towards A Rigorous Science of Interpretable Machine Learning (2017)",
    "Domain expert experiment with a simpler or partial task": "Domain expert experiments with a simplified or partial version of the real task the application has been developed for are based on the idea that the best way to assess the quality of the explanations is to test the system with a real task and the target audience it was built for. The performance of human-agent team can objectively be measured with respect to the goal of the task, such as the accuracy of diagnoses or the rate of identifying errors made by the system. This evaluation method is difficult to perform as it requires high standards of experimental design and requires the budget to compensate domain experts but since it evaluates the system on the intended task with the intended users, it gives strong evidence of success. \nMore information: Towards A Rigorous Science of Interpretable Machine Learning (2017)",
    "Forward simulation": "Forward simulation (also known as a prediction task) is a method in which the human participants are presented with an input and the system’s explanation and must use this information to correctly predict the system’s output. The goal of this method is to test and quantify the user’s understanding of the system. Good explanations should lead to better understanding and therefore better predictability of the system’s future actions. \nMore information: Towards A Rigorous Science of Interpretable Machine Learning (2017)",
    "Counterfactual simulation": "Counterfactual simulation assesses user’s understanding of the system by presenting human participants with an input, an output and the corresponding explanation. Participants must then determine what must be changed to change the current output to a desired different one. \nMore information: Towards A Rigorous Science of Interpretable Machine Learning (2017)",
    "Diagramming task": "A diagramming task is a method, in which human participants create a conceptual diagram to present their knowledge of the system, processes and their relations. It can be used to get users to convey their understanding to researchers. Researchers can then analyze the diagrams and get a deep insight into how users grasp a system and where their flaws are. This can help find areas in which explanations need to be improved. The creation of these diagrams can take users some time but there are many user friendly software systems available to assist them. \nMore information: Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance (2023)",
    "Curiosity Checklist": "The Curiosity Checklist aims to measure the timing and need for an explanation. It is intended to be used with systems that require users to request explanations on their own when they need them, and has the user check all items from the list that apply to why they requested the explanation. Researchers can get insights into how deep the curiosity / confusion is in different situations based on how many items are checked. They can also analyze which specific items are checked to reveal which parts of the system need further explaining and ways in which the explanations could be improved. \nMore information: Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance (2023)",
    "Questioning the user after each provided explanation": "Questioning users each time an explanation was provided by the system is a good way to measure timing and need for an explanation on systems that provide explanations on their own, without users specifically having to request them. Researchers can use this method to find out if the explanation was actually necessary and if the explanation was helpful. \nMore information: A Survey of Evaluation Methods and Metrics for Explanations in Human–Robot Interaction (HRI) (2023)",
    "Situation awareness-based global assessment technique (SAGAT)": "The Situation awareness-based global assessment tech nique (SAGAT) works by letting the human participant work together with the robot in a simulated environment and freezing the experiment at various points to ask a set of questions specific to the user’s task. This allows researchers to check the user’s understanding of the robot, see if the user’s informational needs are met at different points throughout the interaction and detect moments of confusion, in which the explanations are not sufficient. It produces qualitative data that can be analyzed to figure out which parts of the explanations need to be improved and gives a good insight into the mind of the human participants. \nMore information: A Situation Awareness-Based Framework for Design and Evaluation of Explainable AI (2020)",
    "Concurrent think-aloud": "The think-aloud method is a popular way of evaluating the user’s understanding of the system. During concurrent think-aloud problem solving, the user narrates their thought process while interacting with the system. Think-aloud protocols provide very detailed information about the user’s thought process, understanding of the system and times where explanations are not sufficient. However, it results in a very large amount of qualitative data and is therefore very time consuming to analyze. \nMore information: A Survey of Evaluation Methods and Metrics for Explanations in Human–Robot Interaction (HRI) (2023)",
    "Concurrent think-aloud with added question answering": "The think-aloud method with added question answering is a popular way of evaluating the user’s understanding of the system. During concurrent think-aloud problem solving, the user narrates their thought process while interacting with the system. Think-aloud protocols provide very detailed information about the user’s thought process, understanding of the system and times where explanations are not sufficient. However, it results in a very large amount of qualitative data and is therefore very time consuming to analyze. The added questions allow the researcher to probe specific aspect of the user’s thought process but is highly dependent on the skill of the interviewer. \nMore information: Measures for explainable AI: Explanation goodness, user satisfaction, mental models, curiosity, trust, and human-AI performance (2023)",
    "Retrospective think-aloud": "The think-aloud method is a popular way of evaluating the user’s understanding of the system. During retrospective think-aloud problem solving, the user interacts with the system without narrating their thought process and then re-watches the interaction afterwards to narrate their thoughts. Concurrent think-aloud may change the way the user behaves du ring the interaction while retrospective think-aloud may produce results that are less true to the actual thought process. Think-aloud protocols provide very detailed information about the user’s thought process, understanding of the system and times where explanations are not sufficient. However, it results in a very large amount of qualitative data and is therefore very time consuming to analyze. \nMore information: A Survey of Evaluation Methods and Metrics for Explanations in Human–Robot Interaction (HRI) (2023)",
    "Godspeed Questionnaire": "The Godspeed Questionnaire is a standardized measurement tool for human perception of robots on a Likert scale. The questionnaire focuses on the five concepts anthropomorphism (human characteristics and behavior), animacy (making robots lifelike), likeability, perceived intelligence, and perceived safety, since the authors identified those concepts as especially important in HRI. Since the questionnaire includes questions about animacy, the robot should be designed to be lifelike for this method to be used. The Godspeed Questionnaire makes it easy to quantify user perception of robots and compare the effect of explanations. \nMore information: Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots (2009)",
    "Consistency and sufficiency": "Measuring consistency and sufficiency is a method to estimate faithfulness in black box models. Consistency describes how often two inputs with the same explanation lead to the same output, while sufficiency measures that if an output is explained by the agent, other inputs lead to the same output if the same explanation can be applied. \nMore information: Framework for Evaluating Faithfulness of Local Explanations (2022)",
    "Comparison of internal trace and explanation": "If the system’s internal trace is visible, then faithfulness can simply be evaluated by comparing how well the explanations match the system’s actual internal processes. The explanation should be as true to the internal process as possible.",
    "Input Perturbation": "To measure the stability of explanations, one can measure the sensitivity to small irrelevant perturbations to the input. A stable explanation should be able to robustly handle small noise in the input. \nMore information: Better Metrics for Evaluating Explainable Artificial Intelligence (2021)",
    "Number of rules used in an explanation": "If explanations are based on rules, then simplicity can be measured by counting the number of rules used in the explanation and adding a penalty for too many rules, starting at a specific baseline. This can be used to quickly and objectively measure the simplicity of explanations. \nMore information: Better Metrics for Evaluating Explainable Artificial Intelligence (2021)",
    "Number of features inputted to create an explanation": "For neural network based systems, an objective metric to measure simplicity is the number of features inputted to create an explanation. The idea behind this is that the more features are used in an explanation, the more difficult it becomes to understand. \nMore information: Better Metrics for Evaluating Explainable Artificial Intelligence (2021)",
}
